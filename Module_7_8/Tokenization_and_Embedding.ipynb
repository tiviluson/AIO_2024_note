{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f80000a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.17 in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: tqdm in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from torchtext==0.17) (4.67.1)\n",
      "Requirement already satisfied: requests in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from torchtext==0.17) (2.32.3)\n",
      "Requirement already satisfied: torch==2.2.0 in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from torchtext==0.17) (2.2.0+cu121)\n",
      "Requirement already satisfied: numpy in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from torchtext==0.17) (1.26.4)\n",
      "Requirement already satisfied: torchdata==0.7.1 in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from torchtext==0.17) (0.7.1)\n",
      "Requirement already satisfied: filelock in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from torch==2.2.0->torchtext==0.17) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from torch==2.2.0->torchtext==0.17) (4.12.2)\n",
      "Requirement already satisfied: sympy in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from torch==2.2.0->torchtext==0.17) (1.13.1)\n",
      "Requirement already satisfied: networkx in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from torch==2.2.0->torchtext==0.17) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from torch==2.2.0->torchtext==0.17) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from torch==2.2.0->torchtext==0.17) (2024.6.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from torchdata==0.7.1->torchtext==0.17) (2.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from jinja2->torch==2.2.0->torchtext==0.17) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from requests->torchtext==0.17) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from requests->torchtext==0.17) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from requests->torchtext==0.17) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from sympy->torch==2.2.0->torchtext==0.17) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\miniconda3\\envs\\torchtext\\lib\\site-packages (from tqdm->torchtext==0.17) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50690a4d",
   "metadata": {},
   "source": [
    "# Building a Vocabulary and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b583f",
   "metadata": {},
   "source": [
    "1. Build vocabulary\n",
    "1. Build tokenizer that maps sentences to sequences of tokens, including:\n",
    "    1. Split\n",
    "    1. Normalize: lowercase, remove punctuation, etc.\n",
    "    1. Tokenize\n",
    "    1. Pad and Truncate\n",
    "1. Build Embedding layer that maps sequences of tokens to embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d33dd",
   "metadata": {},
   "source": [
    "## With `torchtext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04264506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==2.2.0+cu121\n",
      "torchdata==0.7.1\n",
      "torchtext==0.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | find \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2bf9e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab, build_vocab_from_iterator, Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7934ed",
   "metadata": {},
   "source": [
    "### Build a vocabulary from a frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1d2c72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<unk>', 0),\n",
       " ('<pad>', 1),\n",
       " ('<bos>', 2),\n",
       " ('<eos>', 3),\n",
       " ('hello', 4),\n",
       " ('world', 5),\n",
       " ('this', 6),\n",
       " ('is', 7),\n",
       " ('a', 8),\n",
       " ('test', 9)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "token_frequency_dict = OrderedDict(\n",
    "    {\n",
    "        \"hello\": 1,\n",
    "        \"world\": 5,\n",
    "        \"this\": 1,\n",
    "        \"is\": 3,\n",
    "        \"a\": 2,\n",
    "        \"test\": 1,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "vocabulary = vocab(\n",
    "    token_frequency_dict, specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"], min_freq=1\n",
    ")\n",
    "\n",
    "\n",
    "vocabulary.set_default_index(vocabulary[\"<unk>\"])\n",
    "\n",
    "\n",
    "sorted(vocabulary.get_stoi().items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62908f0c",
   "metadata": {},
   "source": [
    "### Build a vocabulary from a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7b043a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokenizer(\"You can now install TorchText using pip!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37381c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0, '<pad>': 1, 'hello': 2, '!': 3, 'a': 4, 'there': 7, 'test': 6, 'is': 5, 'this': 8, 'world': 9}\n",
      "['<unk>', '<pad>', 'hello', '!', 'a', 'is', 'test', 'there', 'this', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Building a vocabulary from a list of sentences\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "corpus = [\"Hello World\", \"This is a test\", \"Hello there!\"]\n",
    "vocab = build_vocab_from_iterator(yield_tokens(corpus), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "print(vocab.get_stoi())\n",
    "print(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d6911f",
   "metadata": {},
   "source": [
    "### Truncating and Padding Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "514e15ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 9, 3, 8, 5, 4, 0, 0, 0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"Hello world! This is a sample sentence.\"\n",
    "# Tokenize the sample sentence\n",
    "tokens = tokenizer(sample)\n",
    "# Convert tokens to indices using the vocabulary\n",
    "indices = [vocab[token] for token in tokens]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18437435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', '!', 'this', 'is', 'a', '<unk>', '<unk>', '<unk>']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab.get_itos()[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4aeb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 8\n",
    "\n",
    "# Truncate or pad the sequence to the maximum length\n",
    "if len(indices) > MAX_LENGTH:\n",
    "    indices = indices[:MAX_LENGTH]\n",
    "else:\n",
    "    indices += [vocab[\"<pad>\"]] * (MAX_LENGTH - len(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed548d",
   "metadata": {},
   "source": [
    "# With `HuggingFace`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b69c26",
   "metadata": {},
   "source": [
    "## Customized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6525f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Hello World\", \"This is a test\", \"Hello there!\"]\n",
    "sample = \"Hello world! This is a sample sentence.\"\n",
    "MAX_LENGTH = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0ec4f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Hello world! This is a new sentence.\n",
      "Output: Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "Token IDs: [15, 46, 11, 19, 33, 22, 0, 0, 5]\n",
      "Tokens: ['Hello', 'world', '!', 'This', 'is', 'a', '[UNK]', '[UNK]', '.']\n",
      "Decoded text: Hello world ! This is a .\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# 1. Create a sample corpus\n",
    "corpus = [\n",
    "    \"Hello world, how are you?\",\n",
    "    \"This is an example of building a word-level tokenizer.\",\n",
    "    \"We're using Hugging Face tokenizers library!\",\n",
    "    \"Word-level tokenizers split text by whitespace and punctuation.\",\n",
    "    \"They're simple but effective for many NLP tasks.\",\n",
    "]\n",
    "\n",
    "# 2. Initialize a tokenizer with WordLevel model\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))  # type: ignore\n",
    "\n",
    "# 3. Set the pre-tokenizer to split on whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()  # type: ignore\n",
    "\n",
    "# 4. Initialize a trainer\n",
    "trainer = WordLevelTrainer(\n",
    "    min_frequency=1, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]  # type: ignore\n",
    ")\n",
    "\n",
    "# 5. Train the tokenizer\n",
    "tokenizer.train_from_iterator(corpus, trainer)\n",
    "\n",
    "# 6. Add post-processing for adding special tokens\n",
    "# tokenizer.post_processor = TemplateProcessing(\n",
    "#     single=\"[CLS] $A [SEP]\",\n",
    "#     pair=\"[CLS] $A [SEP] $B [SEP]\",\n",
    "#     special_tokens=[\n",
    "#         (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "#         (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# 7. Test the tokenizer\n",
    "test_text = \"Hello world! This is a new sentence.\"\n",
    "output = tokenizer.encode(test_text)\n",
    "\n",
    "print(f\"Input text: {test_text}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"Token IDs: {output.ids}\")\n",
    "print(f\"Tokens: {output.tokens}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(output.ids)}\")\n",
    "\n",
    "# # 8. Save the tokenizer for later use\n",
    "# tokenizer.save(\"word_level_tokenizer.json\")\n",
    "\n",
    "# # 9. Loading a saved tokenizer\n",
    "# loaded_tokenizer = Tokenizer.from_file(\"word_level_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47acb916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PreTrainedTokenizerFast output:\n",
      "Tokenized output: {'input_ids': tensor([[15, 46, 11, 19, 33, 22,  0,  0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Input IDs: tensor([[15, 46, 11, 19, 33, 22,  0,  0]])\n",
      "Tokens: Hello world ! This is a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./my_tokenizer\\\\tokenizer_config.json',\n",
       " './my_tokenizer\\\\special_tokens_map.json',\n",
       " './my_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast  # type: ignore\n",
    "\n",
    "# 10. Wrap with PreTrainedTokenizerFast for use with transformers\n",
    "fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "# Test the wrapped tokenizer\n",
    "# Ref: https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__\n",
    "tokenized_output = fast_tokenizer(\n",
    "    test_text,\n",
    "    padding=\"max_length\",  # require fast_tokenizer.pad_token\n",
    "    truncation=True,\n",
    "    max_length=8,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(\"\\nPreTrainedTokenizerFast output:\")\n",
    "print(f\"Tokenized output: {tokenized_output}\")\n",
    "print(f\"Input IDs: {tokenized_output.input_ids}\")\n",
    "print(\n",
    "    f\"Tokens: {fast_tokenizer.decode(tokenized_output.input_ids[0], skip_special_tokens=True)}\"\n",
    ")\n",
    "\n",
    "# 11. Save the tokenizer (optional)\n",
    "fast_tokenizer.save_pretrained(\"./my_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05f14bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast output:\n",
      "Input IDs shape: torch.Size([7, 8])\n",
      "Tokens: 'This is a [UNK] [UNK] . [PAD] [PAD]' and 'This is a [UNK] [UNK] [UNK] [UNK] [UNK]'\n",
      "Overflowing chunk mapping: tensor([0, 1, 1, 1, 1, 1, 1])\n",
      "Offsets mapping shape: torch.Size([7, 8, 2])\n",
      "Offsets mapping: tensor([[ 0,  4],\n",
      "        [ 5,  7],\n",
      "        [ 8,  9],\n",
      "        [10, 15],\n",
      "        [16, 24],\n",
      "        [24, 25],\n",
      "        [ 0,  0],\n",
      "        [ 0,  0]])\n"
     ]
    }
   ],
   "source": [
    "test_text = [\n",
    "    \"This is a short sentence.\",\n",
    "    \"This is a much longer sentence that will definitely exceed the maximum length and therefore should be split into multiple features by the tokenizer when we ask it to return overflowing tokens.\",\n",
    "]\n",
    "\n",
    "output = fast_tokenizer(\n",
    "    test_text,\n",
    "    padding=\"max_length\",  # require fast_tokenizer.pad_token\n",
    "    truncation=True,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=3,\n",
    "    max_length=8,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(\"PreTrainedTokenizerFast output:\")\n",
    "# print(f\"Tokenized output: {output}\")\n",
    "print(f\"Input IDs shape: {output.input_ids.shape}\")  # 7 chunks of 8 tokens each\n",
    "print(\n",
    "    f\"Tokens: '{fast_tokenizer.decode(output.input_ids[0])}' and '{fast_tokenizer.decode(output.input_ids[1])}'\"\n",
    ")  # stride=3 of overlapping tokens between chunks\n",
    "print(\n",
    "    f\"Overflowing chunk mapping: {output.overflow_to_sample_mapping}\"\n",
    ")  # map chunks to original sentences\n",
    "print(\n",
    "    f\"Offsets mapping shape: {output.offset_mapping.shape}\"\n",
    ")\n",
    "print(\n",
    "    f\"Offsets mapping: {output.offset_mapping[0]}\"\n",
    ")  # map tokens to original text positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42a1d60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b88936c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '!', 'This', 'is', 'a', '[UNK]', '[UNK]', '.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.tokenize(\"Hello world! This is a sample sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c8f56",
   "metadata": {},
   "source": [
    "## Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4fc456fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MAX_LENGTH = 8\n",
    "\n",
    "# Load a pre-trained tokenizer (e.g., BERT)\n",
    "# This will download the tokenizer files if not already cached. Default to Fast tokenizer if available.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a84fdcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Hello world! This is a sample sentence.\n",
      "\n",
      "Tokenizer output:\n",
      "input_ids: tensor([[ 101, 7592, 2088,  999, 2023, 2003, 1037,  102]])\n",
      "token_type_ids: tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Tokens: ['[CLS]', 'hello', 'world', '!', 'this', 'is', 'a', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Process a single sentence\n",
    "sample = \"Hello world! This is a sample sentence.\"\n",
    "\n",
    "encoding = tokenizer(\n",
    "    sample,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(\"Input text:\", sample)\n",
    "print(\"\\nTokenizer output:\")\n",
    "\n",
    "for key, value in encoding.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Decode tokens back to text\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0])\n",
    "print(\"\\nTokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8bf3eac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch encoded corpus shape: torch.Size([3, 8])\n",
      "Custom tokenization vs Hugging Face:\n",
      "Sentence: Hello World\n",
      "Custom tokens: {'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "HF tokens: ['hello', 'world']\n",
      "\n",
      "Sentence: This is a test\n",
      "Custom tokens: {'input_ids': [101, 2023, 2003, 1037, 3231, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "HF tokens: ['this', 'is', 'a', 'test']\n",
      "\n",
      "Sentence: Hello there!\n",
      "Custom tokens: {'input_ids': [101, 7592, 2045, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "HF tokens: ['hello', 'there', '!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process corpus\n",
    "corpus = [\"Hello World\", \"This is a test\", \"Hello there!\"]\n",
    "\n",
    "batch_encoding = tokenizer(\n",
    "    corpus,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(\"Batch encoded corpus shape:\", batch_encoding[\"input_ids\"].shape)\n",
    "# Compare with our custom tokenization\n",
    "print(\"Custom tokenization vs Hugging Face:\")\n",
    "\n",
    "for sentence in corpus:\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Custom tokens: {tokenizer(sentence)}\")\n",
    "    print(f\"HF tokens: {tokenizer.tokenize(sentence)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd5344",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee3adbd",
   "metadata": {},
   "source": [
    "## With `PyTorch` (untrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6fe74a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7cc570e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8712, -0.3465,  0.5327,  0.1636, -0.7755, -0.1996, -1.3155, -1.4636,\n",
       "         -0.4670,  1.7574],\n",
       "        [-0.8712, -0.3465,  0.5327,  0.1636, -0.7755, -0.1996, -1.3155, -1.4636,\n",
       "         -0.4670,  1.7574]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = Embedding(\n",
    "    num_embeddings=len(vocab), embedding_dim=10, padding_idx=vocab[\"<pad>\"]\n",
    ")\n",
    "embedding(torch.tensor([vocab[\"Hello\"], vocab[\"exam\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d947892",
   "metadata": {},
   "source": [
    "## With `torchtext` (pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b50d08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 400,000\n",
      "Embedding dimension: 100\n",
      "Vector for 'hello': tensor([ 0.2669,  0.3963,  0.6169, -0.7745, -0.1039])...\n",
      "Similarity between 'king' and 'queen': 0.7508\n",
      "Similarity between 'man' and 'woman': 0.8323\n",
      "Similarity between 'good' and 'bad': 0.7703\n",
      "Similarity between 'hello' and 'world': 0.2041\n"
     ]
    }
   ],
   "source": [
    "import torchtext.vocab as vocab_utils\n",
    "\n",
    "# Load a pretrained embedding (GloVe)\n",
    "glove = vocab_utils.GloVe(name=\"6B\", dim=100)\n",
    "\n",
    "# Example of using the pretrained embeddings\n",
    "print(f\"Vocabulary size: {len(glove.stoi):,}\")\n",
    "print(f\"Embedding dimension: {glove.vectors.shape[1]}\")\n",
    "\n",
    "# Get vectors for specific words\n",
    "word = \"hello\"\n",
    "if word in glove.stoi:\n",
    "    word_idx = glove.stoi[word]\n",
    "    word_vector = glove.vectors[word_idx]\n",
    "    print(f\"Vector for '{word}': {word_vector[:5]}...\")  # Show first 5 dimensions\n",
    "\n",
    "\n",
    "# Compare similar words using cosine similarity\n",
    "def get_similarity(word1, word2):\n",
    "    if word1 in glove.stoi and word2 in glove.stoi:\n",
    "        vec1 = glove.vectors[glove.stoi[word1]]\n",
    "        vec2 = glove.vectors[glove.stoi[word2]]\n",
    "        return torch.nn.functional.cosine_similarity(\n",
    "            vec1.unsqueeze(0), vec2.unsqueeze(0)\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "# Print some word similarities\n",
    "word_pairs = [(\"king\", \"queen\"), (\"man\", \"woman\"), (\"good\", \"bad\"), (\"hello\", \"world\")]\n",
    "for w1, w2 in word_pairs:\n",
    "    similarity = get_similarity(w1, w2)\n",
    "    if similarity is not None:\n",
    "        print(f\"Similarity between '{w1}' and '{w2}': {similarity.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
