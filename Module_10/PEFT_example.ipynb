{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT with Instruction Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97NYN3e_6BcS"
   },
   "source": [
    "## 1. Install and import necessary libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gO59tP-Bvion",
    "outputId": "e156fce3-6ae9-41ab-c471-0e19d315ea95"
   },
   "outputs": [],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U datasets\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q -U loralib\n",
    "# !pip install -q -U einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1g4cs-qYwjtO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(peft.peft_model.PeftModelForCausalLM,\n",
       " peft.peft_model.PeftModel,\n",
       " transformers.utils.hub.PushToHubMixin,\n",
       " torch.nn.modules.module.Module,\n",
       " object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModelForCausalLM\n",
    "\n",
    "PeftModelForCausalLM.__mro__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b1sJVa39vRct"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import userdata\n",
    "\n",
    "# os.environ['HF_TOKEN'] =  userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YL3aZuZr6LRp"
   },
   "source": [
    "## 2. Prepare pre-trained LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load pre-trained model and quantize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5kFK18GDxMaW"
   },
   "outputs": [],
   "source": [
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    ## Use NormalFloat4b or FP4b\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    ## dtype to dequantize weights into\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "09c07700e8b844ac8e7f3fa39b7fcb55",
      "b83a2cd256c04c699539018f70cb5a7a",
      "f442d04ac9f141d79442ff666b3ec9f8",
      "1338635003604ef9a381ae4e90656d13",
      "6460f5fa30204966940ab5fefa88089b",
      "103394bacaa74355becf76ec07659918",
      "e0b95d0377ab412d8f4969e98fa984ac",
      "66b48f0fea604a7993254d6f9c6e2f76",
      "f5ac4102937f4760a229245b0f358569",
      "225c8653a2414033a39c5e55f4d287a6",
      "7ccd7e29f94247e0ad27ae4d978820c7"
     ]
    },
    "id": "cW56RiCyHp09",
    "outputId": "096afbf7-b0c3-4347-bc3f-e9ff69a5a062"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a905fe90954df0830aa66654bf0f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    offload_state_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZs0QUBIHp09",
    "outputId": "0156f065-a06f-483f-c77d-a8166bc5cdb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "g3IyFZsPP3xC"
   },
   "outputs": [],
   "source": [
    "## 1- Cast the layernorm in fp32\n",
    "## 2- Making output embedding layer require grads (no need for model.enable_input_require_grads())\n",
    "## 3- Add the upcasting of the lm head to fp32\n",
    "## 4- Freezing the base model layers to ensure they are not updated during training\n",
    "## use_gradient_checkpointing: Forward computation in checkpointed regions omits saving tensors for backward and recomputes them during the backward pass\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bTdJED99uf7h",
    "outputId": "6406c911-6a62-47a0-d531-e1f43acae5ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "leNliYBZ0IqT"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    ## output = MHA(q_proj x q, k_proj x k, v_proj x v)\n",
    "    ## output = o_proj x output\n",
    "    ## output = down_proj( act_fn(gate_proj(input)) x up_proj(input) )\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    ## Use Rank-Stablized LoRA, setting scaling to alpha/sqrt(r) instead of alpha/r\n",
    "    # use_rslora=True,\n",
    "    lora_dropout=0.05,\n",
    "    ## [\"none\", \"all\", \"lora_only\"]\n",
    "    bias=\"none\",\n",
    "    ## \"SEQ_CLS\": PeftModelForSequenceClassification,\n",
    "    ## \"SEQ_2_SEQ_LM\": PeftModelForSeq2SeqLM,\n",
    "    ## \"CAUSAL_LM\": PeftModelForCausalLM,\n",
    "    ## \"TOKEN_CLS\": PeftModelForTokenClassification,\n",
    "    ## \"QUESTION_ANS\": PeftModelForQuestionAnswering,\n",
    "    ## \"FEATURE_EXTRACTION\": PeftModelForFeatureExtraction,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "## Freeze the base model parameters\n",
    "model = get_peft_model(model, config, adapter_name=\"sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzKxEualuf7i",
    "outputId": "83f95372-9feb-4c46-efc7-41286b304f13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 24,313,856 || all params: 3,237,063,680 || trainable%: 0.7511\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4LtPfrh4uf7i",
    "outputId": "2c369892-308a-40ea-8092-5fe0651813a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory: 9.74 GB\n",
      "Total memory: 15.99 GB\n"
     ]
    }
   ],
   "source": [
    "mem_free, mem_total = torch.cuda.mem_get_info()\n",
    "print(f\"Free memory: {mem_free / 1024**3:.2f} GB\")\n",
    "print(f\"Total memory: {mem_total / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZFDIeG4uf7i",
    "outputId": "11812cc1-9a1c-4711-858b-81d08f7c1e73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (sentiment_analysis): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (sentiment_analysis): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (sentiment_analysis): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (sentiment_analysis): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (sentiment_analysis): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (sentiment_analysis): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (sentiment_analysis): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (sentiment_analysis): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qK7lUW10a9d5"
   },
   "source": [
    "## 3. Fine-tuning LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKzaGrv2a_fg"
   },
   "source": [
    "### 3.1. Download and inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset: DatasetDict = load_dataset(\n",
    "    \"uitnlp/vietnamese_students_feedback\", split=\"train[:10%]\"  # type: ignore\n",
    ")\n",
    "eval_dataset: DatasetDict = load_dataset(\n",
    "    \"uitnlp/vietnamese_students_feedback\", split=\"validation[:10%]\"  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'sentiment', 'topic'],\n",
       "    num_rows: 1143\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: ['slide giáo trình đầy đủ .', 'nhiệt tình giảng dạy , gần gũi với sinh viên .']\n",
      "sentiment: [2, 2]\n",
      "topic: [1, 0]\n"
     ]
    }
   ],
   "source": [
    "# sentence (str): Text sentence.\n",
    "# sentiment: Sentiment class, with values 0 (negative), 1 (neutral) and 2 (positive).\n",
    "# topic: Topic class, with values 0 (lecturer), 1 (training_program), 2 (facility) and 3 (others).\n",
    "for k, v in train_dataset.to_dict().items():\n",
    "    print(f\"{k}: {v[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(id2label := train_dataset.features[\"sentiment\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0, 'neutral': 1, 'positive': 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(label2id := {v: k for k, v in enumerate(id2label)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT_TEMPLATE = \"\"\"Predict the sentiment of the following input sentence.\n",
    "The response must begin with \"Sentiment: \", followed by one of these keywords: \"positive\", \"negative\", or \"neutral\", to reflect the sentiment of the input sentence.\n",
    "\n",
    "Sentence: {input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e58daa9ca034ffbad0b305715f67128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7481da3b184d0fa8107b295bc5d981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/158 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LENGTH = 200\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    results = {\n",
    "        \"input_ids\": [],\n",
    "        \"labels\": [],\n",
    "        \"attention_mask\": [],\n",
    "    }\n",
    "\n",
    "    for sentence, label in zip(examples[\"sentence\"], examples[\"sentiment\"]):\n",
    "        input = USER_PROMPT_TEMPLATE.format(input=sentence)\n",
    "        output = id2label[label]\n",
    "\n",
    "        input = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. You must fulfill the user request.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": input},\n",
    "        ]\n",
    "        output = input + [{\"role\": \"assistant\", \"content\": output}]\n",
    "\n",
    "        ## apply_chat_template: This is specific to Chat/Intruct models like Llama-3\n",
    "        tokenized_input = tokenizer.apply_chat_template(\n",
    "            input,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=None,  # Return as list, not tensor\n",
    "            truncate=True,\n",
    "            # padding=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "        )\n",
    "        tokenized_input_length = len(tokenized_input)\n",
    "\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            output,\n",
    "            tokenize=True,\n",
    "            return_tensors=None,  # Return as list, not tensor\n",
    "            truncate=True,\n",
    "            ## Padding should have been done by the DataCollator, but it only pads the input_ids, not the labels??\n",
    "            ## https://github.com/huggingface/transformers/issues/31493\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LENGTH,\n",
    "        )\n",
    "\n",
    "        labels = input_ids.copy()\n",
    "        ## Specifically set the first part of the labels to -100 so that it will be ignored in the loss calculation\n",
    "        labels[:tokenized_input_length] = [-100] * tokenized_input_length\n",
    "\n",
    "        ## Padding will be handled by the DataCollator\n",
    "        results[\"input_ids\"].append(input_ids)\n",
    "        results[\"labels\"].append(labels)\n",
    "        results[\"attention_mask\"].append([1] * len(input_ids))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "## By default, SFT trainer only require 1 column: \"text\"\n",
    "## dataset.map CANNOT return tensors and will return lists instead\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=train_dataset.column_names\n",
    ")\n",
    "tokenized_eval_dataset = eval_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=eval_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200]), torch.Size([200]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(tokenized_train_dataset[\"input_ids\"][0]).shape, torch.tensor(\n",
    "    tokenized_train_dataset[\"labels\"][0]\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 Jun 2025\n",
      "\n",
      "You are a helpful assistant. You must fulfill the user request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Predict the sentiment of the following input sentence.\n",
      "The response must begin with \"Sentiment: \", followed by one of these keywords: \"positive\", \"negative\", or \"neutral\", to reflect the sentiment of the input sentence.\n",
      "\n",
      "Sentence: slide giáo trình đầy đủ.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "positive<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_train_dataset[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcpuJU9_bDJu"
   },
   "source": [
    "## 4. Train and evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KgGMT5hruf7k",
    "outputId": "b566ebf8-cb3f-4643-b18b-9cb6453020cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": [\n",
       "    128001,\n",
       "    128008,\n",
       "    128009\n",
       "  ],\n",
       "  \"head_dim\": 128,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 24,\n",
       "  \"num_hidden_layers\": 28,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"quantization_config\": {\n",
       "    \"_load_in_4bit\": true,\n",
       "    \"_load_in_8bit\": false,\n",
       "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "    \"bnb_4bit_quant_type\": \"nf4\",\n",
       "    \"bnb_4bit_use_double_quant\": true,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": true,\n",
       "    \"load_in_8bit\": false,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 32.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.52.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dW973laKADUH"
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    ## Use gradient accumulation to simulate larger batch size\n",
    "    ## (instead of batch_size=16, use batch_size=8 and gradient_accumulation_steps=2)\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    ## Use fp16 to train (5b exponent + 10b fraction)\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./sentiment_analysis_lora\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    ## Already handled by prepare_model_for_kbit_training\n",
    "    # gradient_checkpointing=True,\n",
    "    ## Avoid warning\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    ## Evaluate every 50 steps\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    ## Save every 50 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    ## Save the 3 most recent checkpoints, including the best one (load_best_model_at_end=True)\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    ## Loss function is embedded in the model\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    ## May report to WandB, tensorboard, etc.\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "\n",
    "## TODO: replace with DataCollatorForSeq2Seq\n",
    "## Inputs are dynamically padded to the maximum length of a batch if they are not all of the same length.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    pad_to_multiple_of=8,\n",
    "    # Causal LM does not use Masked LM\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RqJV3qj-uf7k",
    "outputId": "aac9f3e4-5f2c-477c-bd2d-e487cdad2e8e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [142/142 04:57, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.369700</td>\n",
       "      <td>0.404771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.412616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at ./sentiment_analysis_lora\\checkpoint-50\\pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=142, training_loss=0.3197409577772651, metrics={'train_runtime': 299.4516, 'train_samples_per_second': 7.634, 'train_steps_per_second': 0.474, 'total_flos': 7720617125068800.0, 'train_loss': 0.3197409577772651, 'epoch': 1.9790209790209792})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantization-aware training settings\n",
    "## Disable the meaningless KV cache to save memory during training\n",
    "model.config.use_cache = False\n",
    "## Already enabled in prepare_model_for_kbit_training\n",
    "# model.enable_input_require_grads()\n",
    "## This should have been done by prepare_model_for_kbit_training, still raises a warning\n",
    "## https://github.com/huggingface/transformers/issues/28536#issuecomment-2312997910\n",
    "model.gradient_checkpointing_enable({\"use_reentrant\": False})\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,  # type: ignore\n",
    "    eval_dataset=tokenized_eval_dataset,  # type: ignore\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    ## May define subclasses from TrainerCallback\n",
    "    # callbacks=[LogLossCallback()],\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgFsBEbnIUw-"
   },
   "source": [
    "### 4.2 Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "uWMAVb2qfDqj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=10,\n",
    "    # temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    top_k=50,\n",
    "    num_beams=2,\n",
    "    num_return_sequences=2,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.0,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    # bad_words_ids=[]\n",
    ")\n",
    "\n",
    "\n",
    "def predict(model, tokenizer, text, generation_config):\n",
    "    input = USER_PROMPT_TEMPLATE.format(input=text)\n",
    "\n",
    "    input = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. You must fulfill the user request.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": input},\n",
    "    ]\n",
    "\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        input, add_generation_prompt=False, tokenize=False\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            generation_config=generation_config,\n",
    "            # num_beams=2,\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    output_text = [\n",
    "        tokenizer.decode(output, skip_special_tokens=False) for output in outputs\n",
    "    ]\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('giáo trình chưa cụ thể .', 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0][\"sentence\"], eval_dataset[0][\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 Jun 2025\n",
      "\n",
      "You are a helpful assistant. You must fulfill the user request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Predict the sentiment of the following input sentence.\n",
      "The response must begin with \"Sentiment: \", followed by one of these keywords: \"positive\", \"negative\", or \"neutral\", to reflect the sentiment of the input sentence.\n",
      "\n",
      "Sentence: giáo trình chưa cụ thể.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "negative.<|eot_id|>\n",
      "---------------------------------\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 Jun 2025\n",
      "\n",
      "You are a helpful assistant. You must fulfill the user request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Predict the sentiment of the following input sentence.\n",
      "The response must begin with \"Sentiment: \", followed by one of these keywords: \"positive\", \"negative\", or \"neutral\", to reflect the sentiment of the input sentence.\n",
      "\n",
      "Sentence: giáo trình chưa cụ thể.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "negative<|eot_id|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(\n",
    "    *predict(model, tokenizer, eval_dataset[\"sentence\"][0], generation_config),\n",
    "    sep=\"\\n---------------------------------\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9ZTFfNebJDK"
   },
   "source": [
    "## 5. Save model to huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Save the fine-tuned model\n",
    "\n",
    "After fine-tuning, you should save both the adapter weights and configuration to make it easy to reload later. The PEFT library makes this straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained_model\\\\tokenizer_config.json',\n",
       " 'trained_model\\\\special_tokens_map.json',\n",
       " 'trained_model\\\\chat_template.jinja',\n",
       " 'trained_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model adapter weights\n",
    "save_directory = \"trained_model\"\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Optional: Also save the tokenizer for consistency\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Load the fine-tuned model\n",
    "\n",
    "To use your fine-tuned model later, you'll need to load both the base model and the trained adapter weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    ## Use NormalFloat4b or FP4b\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    ## dtype to dequantize weights into\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94be80cab21b4d02af2b4df0397ff775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Define paths\n",
    "saved_adapter_path = \"trained_model/sentiment_analysis\"\n",
    "\n",
    "# 1. Load the PEFT configuration\n",
    "peft_config = PeftConfig.from_pretrained(saved_adapter_path)\n",
    "\n",
    "# 2. Load the base model with quantization settings\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,  # Using the same quantization config as during training\n",
    ")\n",
    "\n",
    "# 3. Load the PEFT adapter weights\n",
    "loaded_model = PeftModel.from_pretrained(base_model, saved_adapter_path)\n",
    "\n",
    "# 4. Load the tokenizer\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "loaded_tokenizer.pad_token = loaded_tokenizer.eos_token  # Set pad token to eos token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Test the loaded model\n",
    "\n",
    "After loading the model, you can test it with the same prediction function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=10,\n",
    "    # temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    top_k=50,\n",
    "    num_beams=2,\n",
    "    num_return_sequences=2,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.0,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    # bad_words_ids=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 Jun 2025\n",
      "\n",
      "You are a helpful assistant. You must fulfill the user request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Predict the sentiment of the following input sentence.\n",
      "The response must begin with \"Sentiment: \", followed by one of these keywords: \"positive\", \"negative\", or \"neutral\", to reflect the sentiment of the input sentence.\n",
      "\n",
      "Sentence: giáo trình chưa cụ thể.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "negative.<|eot_id|>\n",
      "---------------------------------\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 Jun 2025\n",
      "\n",
      "You are a helpful assistant. You must fulfill the user request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Predict the sentiment of the following input sentence.\n",
      "The response must begin with \"Sentiment: \", followed by one of these keywords: \"positive\", \"negative\", or \"neutral\", to reflect the sentiment of the input sentence.\n",
      "\n",
      "Sentence: giáo trình chưa cụ thể.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "negative<|eot_id|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()\n",
    "print(\n",
    "    *predict(\n",
    "        loaded_model, loaded_tokenizer, eval_dataset[\"sentence\"][0], generation_config\n",
    "    ),\n",
    "    sep=\"\\n---------------------------------\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Push model to Hugging Face Hub\n",
    "\n",
    "To share your fine-tuned model with others, you can push it to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()  # Will prompt for your HF token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e3c4aa68074160bbf7068b32bba453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/97.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tiviluson/sentiment-analysis-llama3-lora/commit/e01f9659f43c09003c8965e1af804a8809ef5aff', commit_message='Upload tokenizer', commit_description='', oid='e01f9659f43c09003c8965e1af804a8809ef5aff', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tiviluson/sentiment-analysis-llama3-lora', endpoint='https://huggingface.co', repo_type='model', repo_id='tiviluson/sentiment-analysis-llama3-lora'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"tiviluson/sentiment-analysis-llama3-lora\"\n",
    "model.push_to_hub(model_name)\n",
    "tokenizer.push_to_hub(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Merge adapter weights with base model (Optional)\n",
    "\n",
    "For deployment in production, you might want to merge the LoRA weights back into the base model for faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567bc7b76aae4140bbcc6daeeaca1bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "d:\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\transformers\\modeling_utils.py:3461: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d81cef635254b6d840591ca390792e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('merged-model\\\\tokenizer_config.json',\n",
       " 'merged-model\\\\special_tokens_map.json',\n",
       " 'merged-model\\\\chat_template.jinja',\n",
       " 'merged-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge adapter weights with the base model. This MAY NOT WORK for quantized models.\n",
    "# Create a copy of the model that can be merged\n",
    "\n",
    "# You typically need a model that's not loaded in 4-bit/8-bit for proper merging\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load base model in FP16/BF16\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load adapter\n",
    "from peft import PeftModel\n",
    "\n",
    "saved_adapter_path = \"trained_model/sentiment_analysis\"\n",
    "\n",
    "merged_model = PeftModel.from_pretrained(base_model, saved_adapter_path)\n",
    "\n",
    "# Merge weights\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged-model\")\n",
    "tokenizer.save_pretrained(\"merged-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 Jun 2025\n",
      "\n",
      "You are a helpful assistant. You must fulfill the user request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Predict the sentiment of the following input sentence.\n",
      "The response must begin with \"Sentiment: \", followed by one of these keywords: \"positive\", \"negative\", or \"neutral\", to reflect the sentiment of the input sentence.\n",
      "\n",
      "Sentence: giáo trình chưa cụ thể.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "negative<|eot_id|><|eot_id|>\n",
      "---------------------------------\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 Jun 2025\n",
      "\n",
      "You are a helpful assistant. You must fulfill the user request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Predict the sentiment of the following input sentence.\n",
      "The response must begin with \"Sentiment: \", followed by one of these keywords: \"positive\", \"negative\", or \"neutral\", to reflect the sentiment of the input sentence.\n",
      "\n",
      "Sentence: giáo trình chưa cụ thể.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "negative.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "merged_model.eval()\n",
    "print(\n",
    "    *predict(\n",
    "        merged_model, loaded_tokenizer, eval_dataset[\"sentence\"][0], generation_config\n",
    "    ),\n",
    "    sep=\"\\n---------------------------------\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Manage multiple adapters (Optional)\n",
    "If you want to manage multiple adapters, you can load different adapters for different tasks or datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With one adapter at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is already a PeftModel with one adapter loaded\n",
    "# model.load_adapter(\"./path_to_another_adapter\", adapter_name=\"adapter2\")\n",
    "# model.set_adapter(\"adapter2\") # Switch to adapter2\n",
    "# model.set_adapter(\"default\") # Switch back to the first loaded adapter\n",
    "# model.unload_adapter(\"adapter2\") # Unload adapter2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With multiple adapters at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import PeftMixedModel\n",
    "\n",
    "# base_model = ...  # load the base model, e.g. from transformers\n",
    "# # load first adapter, which will be called \"default\"\n",
    "# peft_model = PeftMixedModel.from_pretrained(base_model, <path_to_adapter1>)\n",
    "# peft_model.load_adapter(<path_to_adapter2>, adapter_name=\"other\")\n",
    "# peft_model.set_adapter([\"default\", \"other\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Disable adapter (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with lora_model.disable_adapter():\n",
    "#     # Inside this context, the model behaves like the original base model\n",
    "#     base_model_outputs = lora_model.generate(...)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09c07700e8b844ac8e7f3fa39b7fcb55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b83a2cd256c04c699539018f70cb5a7a",
       "IPY_MODEL_f442d04ac9f141d79442ff666b3ec9f8",
       "IPY_MODEL_1338635003604ef9a381ae4e90656d13"
      ],
      "layout": "IPY_MODEL_6460f5fa30204966940ab5fefa88089b"
     }
    },
    "103394bacaa74355becf76ec07659918": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1338635003604ef9a381ae4e90656d13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_225c8653a2414033a39c5e55f4d287a6",
      "placeholder": "​",
      "style": "IPY_MODEL_7ccd7e29f94247e0ad27ae4d978820c7",
      "value": " 2/2 [00:07&lt;00:00,  3.40s/it]"
     }
    },
    "1a9755a097e943e49024f4044128c423": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2057512d97e7496b9b8014d746361de9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "225c8653a2414033a39c5e55f4d287a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25bc7c7d289f46da96ee633c7e2d39e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2db0156ec7904d4db5e2f7768b97dacd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "311621b37f79439088b7abb385217380": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2db0156ec7904d4db5e2f7768b97dacd",
      "placeholder": "​",
      "style": "IPY_MODEL_3d549acaf94d44f6a650a1ce283637ff",
      "value": " 97.3M/97.3M [00:07&lt;00:00, 10.2MB/s]"
     }
    },
    "3d549acaf94d44f6a650a1ce283637ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "44d033126bdf4c92848dfa5cfddb1713": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d1d6e49915445bdb823aff50e9f30d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b2bc133f01734215b09dd6eb7717b3db",
       "IPY_MODEL_57edd5f5d0124f3580763b85509bdd58",
       "IPY_MODEL_311621b37f79439088b7abb385217380"
      ],
      "layout": "IPY_MODEL_640beb91f1dd4e5f90ce413cb2ae0030"
     }
    },
    "5760a005a564413daf983eb827f7db82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57edd5f5d0124f3580763b85509bdd58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5760a005a564413daf983eb827f7db82",
      "max": 97307544,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5c0a460e68534565b2271d196fa5ebb7",
      "value": 97307544
     }
    },
    "5c0a460e68534565b2271d196fa5ebb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "640beb91f1dd4e5f90ce413cb2ae0030": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6460f5fa30204966940ab5fefa88089b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66b48f0fea604a7993254d6f9c6e2f76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72d0f71580d84a12aaba22181398ef40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ccd7e29f94247e0ad27ae4d978820c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7de2d6241c60448d9e8bc8b757d3e0b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc94d706de3540c6baf5a62fc4e8b741",
      "placeholder": "​",
      "style": "IPY_MODEL_1a9755a097e943e49024f4044128c423",
      "value": " 24.0/24.0 [00:00&lt;00:00, 2.93kB/s]"
     }
    },
    "8b1ed1dbbbd146e896a643f8ea965377": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb5d90d0e5dc4f4bbcff3f85d57132dd",
       "IPY_MODEL_f84659c158e44214b1b711643ad5bdba",
       "IPY_MODEL_7de2d6241c60448d9e8bc8b757d3e0b4"
      ],
      "layout": "IPY_MODEL_b87f346ed31b4679bcde5219e746d849"
     }
    },
    "a1b83097ae9a47d3a55651b198547829": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d4a113278423487cb8618927e3852f6e",
       "IPY_MODEL_ea0b24b47bd748f0beae109ea2f3a95a",
       "IPY_MODEL_e1bb0bf97d4c49b4a3e98a1c191473ea"
      ],
      "layout": "IPY_MODEL_fc9a8aed4be947129487baa24b2af408"
     }
    },
    "aa59ca189a174c1183c389c684eb1630": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab02f88a674846f28d5a8aa70826b0ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab9b72df0fdf49fb8ed8f167b3ea11e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b2bc133f01734215b09dd6eb7717b3db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b913e29b6fdf4c9b84c4d0b1ce1b1638",
      "placeholder": "​",
      "style": "IPY_MODEL_f35a2ff5ca1147c5b1f3f6bd6a229594",
      "value": "adapter_model.safetensors: 100%"
     }
    },
    "b83a2cd256c04c699539018f70cb5a7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_103394bacaa74355becf76ec07659918",
      "placeholder": "​",
      "style": "IPY_MODEL_e0b95d0377ab412d8f4969e98fa984ac",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "b87f346ed31b4679bcde5219e746d849": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b913e29b6fdf4c9b84c4d0b1ce1b1638": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb5d90d0e5dc4f4bbcff3f85d57132dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab02f88a674846f28d5a8aa70826b0ca",
      "placeholder": "​",
      "style": "IPY_MODEL_72d0f71580d84a12aaba22181398ef40",
      "value": "README.md: 100%"
     }
    },
    "cc94d706de3540c6baf5a62fc4e8b741": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4a113278423487cb8618927e3852f6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f21ed64b5b9b4a849d20a8f554ee8331",
      "placeholder": "​",
      "style": "IPY_MODEL_f952f3d5b5e444f4b80bfe981a06b234",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "e0b95d0377ab412d8f4969e98fa984ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1bb0bf97d4c49b4a3e98a1c191473ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa59ca189a174c1183c389c684eb1630",
      "placeholder": "​",
      "style": "IPY_MODEL_44d033126bdf4c92848dfa5cfddb1713",
      "value": " 2/2 [00:06&lt;00:00,  3.12s/it]"
     }
    },
    "ea0b24b47bd748f0beae109ea2f3a95a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25bc7c7d289f46da96ee633c7e2d39e8",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab9b72df0fdf49fb8ed8f167b3ea11e5",
      "value": 2
     }
    },
    "eac1a2b08e404fce9e5974e665b73fe6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f21ed64b5b9b4a849d20a8f554ee8331": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f35a2ff5ca1147c5b1f3f6bd6a229594": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f442d04ac9f141d79442ff666b3ec9f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66b48f0fea604a7993254d6f9c6e2f76",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f5ac4102937f4760a229245b0f358569",
      "value": 2
     }
    },
    "f5ac4102937f4760a229245b0f358569": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f84659c158e44214b1b711643ad5bdba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2057512d97e7496b9b8014d746361de9",
      "max": 24,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eac1a2b08e404fce9e5974e665b73fe6",
      "value": 24
     }
    },
    "f952f3d5b5e444f4b80bfe981a06b234": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc9a8aed4be947129487baa24b2af408": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
