{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.4000]])\n",
      "tensor([[2.4000]])\n",
      "tensor([[ 0.7200, -0.9600]])\n",
      "tensor(0.1709)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input tensor\n",
    "x = torch.tensor([[3.0]])\n",
    "\n",
    "# Target labels\n",
    "y = torch.tensor([0])\n",
    "\n",
    "# Weights for the first layer\n",
    "bias1 = torch.tensor([0.0])\n",
    "weight1 = torch.tensor([[0.8]])\n",
    "\n",
    "# Weights for the second layer\n",
    "bias2 = torch.tensor([0.0, 0.0])\n",
    "weight2 = torch.tensor([[0.3, -0.4]])\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Perform forward pass\n",
    "for _ in range(1):\n",
    "    # Compute hidden layer\n",
    "    hidden = torch.matmul(x, weight1) + bias1\n",
    "    print(hidden)\n",
    "\n",
    "    hidden = F.relu(hidden)\n",
    "    print(hidden)\n",
    "\n",
    "    # Compute output layer\n",
    "    output = torch.matmul(hidden, weight2) + bias2\n",
    "    print(output)\n",
    "\n",
    "    # Compute loss\n",
    "    l_value = loss_fn(output, y)\n",
    "    print(l_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quangvinh/miniconda3/envs/torchenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8429, 0.1571]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax()(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1709])\n"
     ]
    }
   ],
   "source": [
    "# Compute -log for 0.8429\n",
    "value = torch.tensor([0.8429])\n",
    "neg_log_value = -torch.log(value)\n",
    "print(neg_log_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer (before ReLU):\n",
      " tensor([[2.4000]], grad_fn=<AddmmBackward0>)\n",
      "Hidden layer (after ReLU):\n",
      " tensor([[2.4000]], grad_fn=<ReluBackward0>)\n",
      "Output layer (before softmax):\n",
      " tensor([[ 0.7200, -0.9600]], grad_fn=<AddmmBackward0>)\n",
      "Loss: tensor(0.1709, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input tensor\n",
    "x = torch.tensor([[3.0]])\n",
    "\n",
    "# Target labels\n",
    "y = torch.tensor([0])\n",
    "\n",
    "# Define the model using nn.Linear and nn.ReLU\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()        \n",
    "        self.layer1 = nn.Linear(2, 2)\n",
    "        self.layer2 = nn.Linear(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hidden layer\n",
    "        hidden = self.layer1(x)\n",
    "        print(\"Hidden layer (before ReLU):\\n\", hidden)\n",
    "\n",
    "        hidden = self.relu(hidden)\n",
    "        print(\"Hidden layer (after ReLU):\\n\", hidden)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.layer2(hidden)\n",
    "        print(\"Output layer (before softmax):\\n\", output)        \n",
    "\n",
    "        return output\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleModel()\n",
    "\n",
    "# Manually set weights and biases\n",
    "model.layer1.weight.data = torch.tensor([[0.8]])\n",
    "model.layer1.bias.data = torch.tensor([0.0])\n",
    "model.layer2.weight.data = torch.tensor([[0.3],\n",
    "                                         [-0.4]])\n",
    "model.layer2.bias.data = torch.tensor([0.0, 0.0])\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Perform forward pass and compute loss\n",
    "output = model(x)\n",
    "l_value = loss_fn(output, y)\n",
    "print(\"Loss:\", l_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "model.zero_grad()\n",
    "l_value.backward()\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.8033]], requires_grad=True)\n",
      "tensor([[-0.3299]])\n",
      "Parameter containing:\n",
      "tensor([0.0011], requires_grad=True)\n",
      "tensor([-0.1100])\n"
     ]
    }
   ],
   "source": [
    "print(model.layer1.weight)\n",
    "print(model.layer1.weight.grad)\n",
    "print(model.layer1.bias)\n",
    "print(model.layer1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3038],\n",
      "        [-0.4038]], requires_grad=True)\n",
      "tensor([[-0.3770],\n",
      "        [ 0.3770]])\n",
      "Parameter containing:\n",
      "tensor([ 0.0016, -0.0016], requires_grad=True)\n",
      "tensor([-0.1571,  0.1571])\n"
     ]
    }
   ],
   "source": [
    "print(model.layer2.weight)\n",
    "print(model.layer2.weight.grad)\n",
    "print(model.layer2.bias)\n",
    "print(model.layer2.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11230000000000001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.157*0.3 - 0.163*0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.15700000000000003"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.843-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10996666666666667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.3299/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
